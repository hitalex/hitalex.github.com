---
layout: post
title: 高斯混合模型和期望最大化算法
categories: 
- 
tags: 
- GMM
- EM
---

# 概述
在这篇博文中，我们将讨论高斯混合模型（Gaussian Mixture Models，GMM）以及它的求解方法--期望最大化（Expectation-Maximization，EM）算法。GMM是经典的无监督聚类模型，它假设数据是从多个高斯分布中得到。我们在GMM的模型求解过程中引入“隐变量”（latent variable）的概念，并基于此使用EM算法求解模型参数。GMM在语音识别技术中有应用。

# 求解高斯混合模型：初尝败果
在线性判别分析（Linear Discriminant Analysis, LDA）中，我们假设每类中的数据服从高斯分布，通过计算两类之间的判别超平面得到分类函数。注意，在线性判别分析中，数据是有类别标签的，也就是属于有监督的分类问题。GMM与线性判别分析模型具有同样的数据生成假设，不同的是，GMM的训练数据中没有类别标签，即为无监督模型。一般地，GMM的定义如下：

$$
p(\mathbf{\mbox{x}}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{\mbox{x}} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
$$

其中，$$\mathcal{N}$$表示（多元）高斯分布，$$K$$表示混合高斯的个数，$$\pi_k$$表示混合参数（mixing coefficient），满足$$0 \le \pi_k \le 1$$和$$\sum_{k=1}^K \pi_k = 1$$。我们首先尝试使用最大似然方法求解，那么目标就是最大化下式：

$$
\ln p(\mathbf{\mbox{X}}|\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma}) = \sum_{n=1}^{N} \ln \left\{ \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{\mbox{x}}_n | \mathbf{\mu}_k, \mathbf{\Sigma}_k) \right\}
$$

如果此时试图使用[拉格朗日乘数法](http://en.wikipedia.org/wiki/Lagrange_multiplier)来求解，那么遇到的最大麻烦将是$$\ln \{ \cdots \}$$这一部分，导致根本无法继续求解。

# 引入隐变量
接下来，我们将引入“隐变量”。我们假设，对于数据点$$\mathbf{\mbox{x}}$$来说存在一个隐变量$$\mathbf{\mbox{z}}$$，也就意味着对于每个数据点的取值都存在一个相应的隐变量的取值，但是无法观测得到。隐变量$$\mathbf{\mbox{z}}$$的定义如下：$$\mathbf{\mbox{z}}$$为$$K$$维变量，满足$$z_k \in \{0, 1\}$$而且$$\sum_k z_k = 1$$，也就意味着隐变量$$\mathbf{\mbox{z}}$$中有且仅有一位为1，其他位置都为0。$$\mathbf{\mbox{z}}$$和$$\pi_k$$的关系如下：

$$
p(z_k = 1) = \pi_k
$$

那么，

$$
p(\mathbf{\mbox{z}}) = \prod_k^K \pi_k^{z_k}
$$

注意，这是一个向量的概率密度定义，你可能需要仔细琢磨一下其中的含义。隐变量$$\mathbf{\mbox{z}}$$的含义为：如果$$z_k = 1$$，那么表示观测值$$\mathbf{\mbox{x}}$$就来自第$$k$$个聚类（cluster）。基于以上定义，可以得到：

$$
p(\mathbf{\mbox{x}}|z_k = 1) = \mathcal{N}(\mathbf{\mbox{x}} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
$$

以及

$$
p(\mathbf{\mbox{x}} | \mathbf{\mbox{z}}) = \prod_{k=1}^K \mathcal{N} (\mathbf{\mbox{x}} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)^{z_k}
$$

读者可以尝试利用已经得到的$$p(\mathbf{\mbox{z}})$$和$$p(\mathbf{\mbox{x}} \vert \mathbf{\mbox{z}})$$重新表示$$p(\mathbf{\mbox{x}})$$，验证下式：

$$
p(\mathbf{\mbox{x}}) = \sum_{\mathbf{\mbox{z}}} p(\mathbf{\mbox{z}}) p(\mathbf{\mbox{x}} | \mathbf{\mbox{z}}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{\mbox{x}} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
$$

# EM算法登场
我们首先给出EM算法的一般步骤，然后将其应用于GMM的参数求解。
## EM算法的一般步骤
给定观测数据$$\mathbf{\mbox{X}}$$，其中第$$n$$行$$\mathbf{\mbox{x}}_n^T$$表示第$$n$$个观测数据，列的个数为数据的维数。正如我们之前所介绍的，每个观测数据都对应一个隐变量的取值--采用$$\mathbf{\mbox{Z}}$$表示，$$\mathbf{\mbox{z}}_n^T$$表示对应$$\mathbf{\mbox{x}}_n^T$$的隐变量取值，经常称$$\{ \mathbf{\mbox{X,Z}} \}$$为完整数据（complete data）。在加入隐变量之后，似然函数变为（$$\mathbf{\theta}$$表示模型的未知参数集合）：

$$
\ln p(\mathbf{\mbox{X}} | \mathbf{\theta}) = \ln \left\{ \sum_{\mathbf{\mbox{z}}} p(\mathbf{\mbox{X}}, \mathbf{\mbox{Z}} | \mathbf{\theta}) \right\}
$$

直接使用上式对参数$$\mathbf{\theta}$$求导还是会遇到前面所遇到的问题，接下来介绍EM算法如何求解。

假定已经给定了一组模型参数的初始取值$$\mathbf{\theta}^{\mbox{old}}$$，我们可以先计算隐变量$$\mathbf{\mbox{Z}}$$的后验概率，即$$p(\mathbf{\mbox{Z}} \vert \mathbf{\mbox{X}}, \mathbf{\theta}^{\mbox{old}})$$。因为$$p(\mathbf{\mbox{X}} \vert \mathbf{\theta})$$的形式较为复杂，我们转而考虑完整数据的似然：$$p(\mathbf{\mbox{X}}, \mathbf{\mbox{Z}} \vert \mathbf{\theta})$$。

EM算法的E-step如下，计算：

$$
\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}}) = \sum_{\mathbf{\mbox{Z}}} p(\mathbf{\mbox{Z}} | \mathbf{\mbox{X}}, \mathbf{\theta}^{\mbox{old}}) \ln p(\mathbf{\mbox{X}}, \mathbf{\mbox{Z}} | \mathbf{\theta})
$$

对于E-step，有以下几点需要注意：

* $$\mathcal{Q}$$本质上是关于变量$$\mathbf{\mbox{Z}}$$的期望，即$$\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}}) = \mathbb{E}_{\mathbf{\mbox{z}}} [\ln p(\mathbf{\mbox{X}}), \mathbf{\mbox{Z}} \vert \mathbf{\theta} ] $$，其中$$\mathbf{\mbox{Z}}$$的概率密度由$$p(\mathbf{\mbox{Z}} \vert \mathbf{\mbox{X}}, \mathbf{\theta}^{\mbox{old}})$$给定（在这里我们假设$$\mathbf{\mbox{Z}}$$是离散变量，如果$$\mathbf{\mbox{Z}}$$是连续值，那么仅需要将期望中的求和改为积分便可）。

* $$\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}})$$中，$$\mathbf{\theta}$$是变量，而$$\mathbf{\theta}^{\mbox{old}}$$是已经给定参数值。由于EM是迭代求解算法，这里的$$\mathbf{\theta}^{\mbox{old}}$$实际上是上一轮中模型参数的取值。

* 一般来说，模型中的参数不止一个，即$$\mathbf{\theta}$$是各个参数构成的向量（此处假设各个参数之间是独立的）。在实际应用中则需要分别计算不同参数的$$\mathcal{Q} (\mathbf{\theta}_i, \mathbf{\theta}_{\neg i}, \mathbf{\theta}^{\mbox{old}})$$。

EM算法的M-step为：

$$
\mathbf{\theta}^{\mbox{new}} = \mbox{argmax}_{\mathbf{\theta}} \mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}})
$$

对于M-step，有以下几点需要注意：

* M-step实际上是一个优化问题，所以必须要求$$\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}})$$的形式不能太复杂，也就意味着$$\mathbf{\mbox{Z}}$$的后验概率以及$$\{ \mathbf{\mbox{X}},\mathbf{\mbox{Z}} \}$$的全概率公式的形式不能太复杂。

* 由于模型参数通常有其他约束，例如GMM中的$$\pi_k$$，所以求解优化算法时通常使用拉格朗日乘数法。


