---
layout: post
title: 高斯混合模型和期望最大化算法
categories: 
- 
tags: 
- GMM
- EM
---

# 概述
在这篇博文中，我们将讨论高斯混合模型（Gaussian Mixture Models，GMM）以及它的求解方法--期望最大化（Expectation-Maximization，EM）算法。GMM是经典的无监督聚类模型，它假设数据是从多个高斯分布中得到。我们在GMM的模型求解过程中引入“隐变量”（latent variable）的概念，并基于此使用EM算法求解模型参数。GMM在语音识别技术中有应用。

# 求解高斯混合模型：初尝败果
在线性判别分析（Linear Discriminant Analysis, LDA）中，我们假设每类中的数据服从高斯分布，通过计算两类之间的判别超平面得到分类函数。注意，在线性判别分析中，数据是有类别标签的，也就是属于有监督的分类问题。GMM与线性判别分析模型具有同样的数据生成假设，不同的是，GMM的训练数据中没有类别标签，即为无监督模型。一般地，GMM的定义如下：

$$
p(\mathbf{\mbox{x}}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{\mbox{x}} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
$$

其中，$$\mathcal{N}$$表示（多元）高斯分布，$$K$$表示混合高斯的个数，$$\pi_k$$表示混合参数（mixing coefficient），满足$$0 \le \pi_k \le 1$$和$$\sum_{k=1}^K \pi_k = 1$$。我们首先尝试使用最大似然方法求解，那么目标就是最大化下式：

$$
\ln p(\mathbf{\mbox{X}}|\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma}) = \sum_{n=1}^{N} \ln \left\{ \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{\mbox{x}}_n | \mathbf{\mu}_k, \mathbf{\Sigma}_k) \right\}
$$

如果此时试图使用[拉格朗日乘数法](http://en.wikipedia.org/wiki/Lagrange_multiplier)来求解，那么遇到的最大麻烦将是$$\ln \{ \cdots \}$$这一部分，导致根本无法继续求解。

# 引入隐变量
接下来，我们将引入“隐变量”。我们假设，对于数据点$$\mathbf{\mbox{x}}$$来说存在一个隐变量$$\mathbf{\mbox{z}}$$，也就意味着对于每个数据点的取值都存在一个相应的隐变量的取值，但是无法观测得到。隐变量$$\mathbf{\mbox{z}}$$的定义如下：$$\mathbf{\mbox{z}}$$为$$K$$维变量，满足$$z_k \in \{0, 1\}$$而且$$\sum_k z_k = 1$$，也就意味着隐变量$$\mathbf{\mbox{z}}$$中有且仅有一位为1，其他位置都为0。$$\mathbf{\mbox{z}}$$和$$\pi_k$$的关系如下：

$$
p(z_k = 1) = \pi_k
$$

那么，

$$
p(\mathbf{\mbox{z}}) = \prod_k^K \pi_k^{z_k}
$$

注意，这是一个向量的概率密度定义，你可能需要仔细琢磨一下其中的含义。隐变量$$\mathbf{\mbox{z}}$$的含义为：如果$$z_k = 1$$，那么表示观测值$$\mathbf{\mbox{x}}$$就来自第$$k$$个聚类（cluster）。基于以上定义，可以得到：

$$
p(\mathbf{\mbox{x}}|z_k = 1) = \mathcal{N}(\mathbf{\mbox{x}} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
$$

以及

$$
p(\mathbf{\mbox{x}} | \mathbf{\mbox{z}}) = \prod_{k=1}^K \mathcal{N} (\mathbf{\mbox{x}} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)^{z_k}
$$

读者可以尝试利用已经得到的$$p(\mathbf{\mbox{z}})$$和$$p(\mathbf{\mbox{x}} \vert \mathbf{\mbox{z}})$$重新表示$$p(\mathbf{\mbox{x}})$$，验证下式：

$$
p(\mathbf{\mbox{x}}) = \sum_{\mathbf{\mbox{z}}} p(\mathbf{\mbox{z}}) p(\mathbf{\mbox{x}} | \mathbf{\mbox{z}}) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{\mbox{x}} | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
$$

# EM算法登场
我们首先给出EM算法的一般步骤，然后将其应用于GMM的参数求解。

## EM算法的一般步骤
给定观测数据$$\mathbf{\mbox{X}}$$，其中第$$n$$行$$\mathbf{\mbox{x}}_n^T$$表示第$$n$$个观测数据，列的个数为数据的维数。正如我们之前所介绍的，每个观测数据都对应一个隐变量的取值--采用$$\mathbf{\mbox{Z}}$$表示，$$\mathbf{\mbox{z}}_n^T$$表示对应$$\mathbf{\mbox{x}}_n^T$$的隐变量取值，经常称$$\{ \mathbf{\mbox{X,Z}} \}$$为完整数据（complete data）。在加入隐变量之后，似然函数变为（$$\mathbf{\theta}$$表示模型的未知参数集合）：

$$
\ln p(\mathbf{\mbox{X}} | \mathbf{\theta}) = \ln \left\{ \sum_{\mathbf{\mbox{z}}} p(\mathbf{\mbox{X}}, \mathbf{\mbox{Z}} | \mathbf{\theta}) \right\}
$$

直接使用上式对参数$$\mathbf{\theta}$$求导还是会遇到前面所遇到的问题，接下来介绍EM算法如何求解。

假定已经给定了一组模型参数的初始取值$$\mathbf{\theta}^{\mbox{old}}$$，我们可以先计算隐变量$$\mathbf{\mbox{Z}}$$的后验概率，即$$p(\mathbf{\mbox{Z}} \vert \mathbf{\mbox{X}}, \mathbf{\theta}^{\mbox{old}})$$。因为$$p(\mathbf{\mbox{X}} \vert \mathbf{\theta})$$的形式较为复杂，我们转而考虑完整数据的似然：$$p(\mathbf{\mbox{X}}, \mathbf{\mbox{Z}} \vert \mathbf{\theta})$$。

EM算法的E-step如下，计算：

$$
\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}}) = \sum_{\mathbf{\mbox{Z}}} p(\mathbf{\mbox{Z}} | \mathbf{\mbox{X}}, \mathbf{\theta}^{\mbox{old}}) \ln p(\mathbf{\mbox{X}}, \mathbf{\mbox{Z}} | \mathbf{\theta})
$$

对于E-step，有以下几点需要注意：

* $$\mathcal{Q}$$本质上是关于变量$$\mathbf{\mbox{Z}}$$的期望，即$$\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}}) = \mathbb{E}_{\mathbf{\mbox{z}}} [\ln p(\mathbf{\mbox{X}}), \mathbf{\mbox{Z}} \vert \mathbf{\theta} ] $$，其中$$\mathbf{\mbox{Z}}$$的概率密度由$$p(\mathbf{\mbox{Z}} \vert \mathbf{\mbox{X}}, \mathbf{\theta}^{\mbox{old}})$$给定（在这里我们假设$$\mathbf{\mbox{Z}}$$是离散变量，如果$$\mathbf{\mbox{Z}}$$是连续值，那么仅需要将期望中的求和改为积分便可）。

* $$\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}})$$中，$$\mathbf{\theta}$$是变量，而$$\mathbf{\theta}^{\mbox{old}}$$是已经给定参数值。由于EM是迭代求解算法，这里的$$\mathbf{\theta}^{\mbox{old}}$$实际上是上一轮中模型参数的取值。

* 一般来说，模型中的参数不止一个，即$$\mathbf{\theta}$$是各个参数构成的向量（此处假设各个参数之间是独立的）。在实际应用中则需要分别计算不同参数的$$\mathcal{Q} (\mathbf{\theta}_i, \mathbf{\theta}_{\neg i}, \mathbf{\theta}^{\mbox{old}})$$。


EM算法的M-step为：

$$
\mathbf{\theta}^{\mbox{new}} = \mbox{argmax}_{\mathbf{\theta}} \mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}})
$$

对于M-step，有以下几点需要注意：

* M-step实际上是一个优化问题，所以必须要求$$\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}})$$的形式不能太复杂，也就意味着$$\mathbf{\mbox{Z}}$$的后验概率以及$$\{ \mathbf{\mbox{X}},\mathbf{\mbox{Z}} \}$$的全概率公式的形式不能太复杂。

* 由于模型参数通常有其他约束，例如GMM中的$$\pi_k$$，所以求解优化算法时通常使用拉格朗日乘数法。

## 将EM算法应用于GMM
首先，根据$$p(\mathbf{\mbox{z}})$$和$$p(\mathbf{\mbox{x}} \vert \mathbf{\mbox{z}})$$计算$$p(\mathbf{\mbox{x,z}})$$，进而计算完整数据$$\{\mathbf{\mbox{X,Z}}\}$$的似然。计算后可得：

$$
p(\mathbf{\mbox{X,Z}} \vert \mathbf{\mu, \Sigma, \pi}) = \prod_{n=1}^N \prod_{k=1}^K \pi_k^{z_{nk}} \mathcal{N}(\mathbf{\mbox{x}}_n \vert \mathbf{\mu_k, \Sigma_k})^{z_{nk}}
$$

将上式取log后的结果为：

$$
\ln p(\mathbf{\mbox{X,Z}} \vert \mathbf{\mu, \Sigma, \pi}) = \sum_{n=1}^N \sum_{k=1}^K z_{nk} \{ \ln \pi_k + \ln \mathcal{N}(\mathbf{\mbox{x}}_n \vert \mathbf{\mu_k, \Sigma_k}) \}
$$

根据贝叶斯公式，由$$p(\mathbf{\mbox{X,Z}} \vert \mathbf{\mu, \Sigma, \pi})$$可以得到$$\mathbf{\mbox{Z}}$$的后验概率（除以 $$p(\mathbf{\mbox{X}} \vert \mathbf{\mu, \Sigma, \pi})$$）：

$$
p(\mathbf{\mbox{Z} \vert \mbox{X}}, \mathbf{\mu, \Sigma, \pi}) \propto \prod_{n=1}^N \prod_{k=1}^K [ \pi_k \mathcal{N}(\mathbf{\mbox{x}}_n \vert \mathbf{\mu_k, \Sigma_k})]^{z_{nk}}
$$

从上式中可以看出，$$\{ \mathbf{\mbox{z}}_n \}$$之间是相互独立的。回想EM算法的E-step，接下来我们利用$$\ln p(\mathbf{\mbox{X,Z}} \vert \mathbf{\mu, \Sigma, \pi})$$和$$p(\mathbf{\mbox{Z} \vert \mbox{X}}, \mathbf{\mu, \Sigma, \pi})$$来计算$$\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}})$$，具体过程如下：

$$
\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}})  =  \sum_{\mathbf{\mbox{Z}}} p(\mathbf{\mbox{Z}} | \mathbf{\mbox{X}}, \mathbf{\theta}^{\mbox{old}}) \ln p(\mathbf{\mbox{X}}, \mathbf{\mbox{Z}} | \mathbf{\theta})
$$

$$
 =  \sum_{\mathbf{\mbox{Z}}} p(\mathbf{\mbox{Z}} \vert \mathbf{\mbox{X}}, \mathbf{\mu, \Sigma, \pi}) \left( \sum_{n=1}^N \sum_{k=1}^K z_{nk} \{ \ln \pi_k + \ln \mathcal{N}(\mathbf{\mbox{x_n}} \vert \mathbf{\mu}_k, \mathbf{\Sigma}_k) \} \right)
$$
 
$$
 =  \sum_{n=1}^N \sum_{k=1}^K \sum_{z_{nk}} p(z_{nk} \vert \mathbf{\mbox{X}}, \mathbf{\mu, \Sigma, \pi}) z_{nk} \{ \ln \pi_k + \ln \mathcal{N}(\mathbf{\mbox{x}}_n \vert \mathbf{\mu}_k, \mathbf{\Sigma}_k) \}
 $$
 
$$
 =  \sum_{n=1}^N \sum_{k=1}^K \mathbb{E}[z_{nk}] \{ \ln \pi_k + \ln \mathcal{N}(\mathbf{\mbox{x}}_n \vert \mathbf{\mu}_k, \mathbf{\Sigma}_k) \}
$$

其中，$$\mathbb{E}[z_{nk}]$$的计算如下：

$$
\mathbb{E}[z_{nk}] = \frac{\Sigma_{z_{nk}} z_{nk} [\pi_k \mathcal{N}(\mathbf{\mbox{x}}_n \vert \mathbf{\mu}_k, \mathbf{\Sigma}_k)]^{z_{nk}}} {\Sigma_{z_{nj}}[\pi_j \mathcal{N}(\mathbf{\mbox{x}}_n \vert \mathbf{\mu}_j, \mathbf{\Sigma}_j)]^{z_{nj}}} = \gamma(z_{nk})
$$

此时，计算$$\gamma(z_{nk})$$已经成为EM算法迭代的一个中间变量，稍后关于其他参数的迭代计算都会依赖这个值。另外，计算这个中间变量时使用的是上一轮中的参数，即$$\theta^{\mbox{old}}$$。

接下来是EM算法中的M-step：

$$
\mathcal{Q} (\mathbf{\theta}, \mathbf{\theta}^{\mbox{old}}) = \mathbb{E}_{\mathbf{\mbox{z}}} [\ln p(\mathbf{\mbox{X}}, \mathbf{\mbox{Z}} \vert \mathbf{\theta}) ] \\
= \gamma(z_{nk}) \{ \ln \pi_k + \ln \mathcal{N}(\mathbf{\mbox{x}}_n \vert \mathbf{\mu}_k, \mathbf{\Sigma}_k) \}
$$

在得到上式后便可以使用拉格朗日乘数法分别对$$\pi_k, \mathbf{\mu}_k, \mathbf{\Sigma}_k$$偏导求解，具体过程略过。*强烈*建议读者自行推导这个优化过程，这其中涉及到拉格朗日乘数法、多元高斯概率密度函数对均值和协方差矩阵的求导等--你真的可能不了解其中的一些细节，除非你自己推导过n遍。

在经历了较为繁琐的推导过程后，你将发现结果的形式却相对简单。我们定义：$$N_k = \sum_{n=1}^N\gamma(z_{nk})$$。其含义可以理解为：平均而言，第$$k$$个聚类中所含有的样本的个数。M-step中其他参数的迭代计算方法如下：

$$
\mathbf{\mu}_k^{\mbox{new}} = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) \mathbf{\mbox{x}}_n
$$

$$
\mathbf{\Sigma}_k^{\mbox{new}} = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) (\mathbf{\mbox{x}}_n - \mathbf{\mu}_k^{\mbox{new}}) (\mathbf{\mbox{x}}_n - \mathbf{\mu}_k^{\mbox{new}})^T
$$

$$
\mathbf{\pi}_k^{\mbox{new}} = \frac{N_k}{N}
$$
